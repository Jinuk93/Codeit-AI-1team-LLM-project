# from llama_cpp import Llama
from typing import Optional, Dict, Any, List
import logging
import time
import os

from src.utils.config import RAGConfig
from src.router.query_router import QueryRouter
from src.prompts.dynamic_prompts import PromptManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class GGUFGenerator:
    """
    GGUF ê¸°ë°˜ Llama-3 ìƒì„±ê¸°
    
    llama.cppë¥¼ ì‚¬ìš©í•˜ì—¬ GGUF í¬ë§· ëª¨ë¸ì„ ë¡œë“œí•˜ê³ 
    ì…ì°° ê´€ë ¨ ì§ˆì˜ì‘ë‹µì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    
    def __init__(
        self,
        model_path: str,
        n_gpu_layers: int = 0,
        n_ctx: int = 2048,
        n_threads: int = 8,
        config = None,
        max_new_tokens: int = 256,
        temperature: float = 0.7,
        top_p: float = 0.9,
        system_prompt: str = "ë‹¹ì‹ ì€ RFP(ì œì•ˆìš”ì²­ì„œ) ë¶„ì„ ë° ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
    ):
        """
        ìƒì„±ê¸° ì´ˆê¸°í™”
        
        Args:
            model_path: GGUF ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            n_gpu_layers: GPUì— ì˜¬ë¦´ ë ˆì´ì–´ ìˆ˜ (0 = CPUë§Œ, 35 = ì „ì²´ GPU)
            n_ctx: ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
            n_threads: CPU ìŠ¤ë ˆë“œ ìˆ˜
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒì„± ë‹¤ì–‘ì„± (0.0~1.0)
            top_p: Nucleus sampling íŒŒë¼ë¯¸í„°
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        """
        self.config = config or RAGConfig() 
        self.model_path = model_path
        self.n_gpu_layers = n_gpu_layers
        self.n_ctx = n_ctx
        self.n_threads = n_threads
        self.max_new_tokens = max_new_tokens
        self.temperature = temperature
        self.top_p = top_p
        self.system_prompt = system_prompt
        
        # ëª¨ë¸ (ë‚˜ì¤‘ì— ë¡œë“œ)
        self.model = None
        
        logger.info(f"GGUFGenerator ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_model(self) -> None:
        """
        GGUF ëª¨ë¸ ë¡œë“œ
        
        ë¡œì§:
        1. USE_MODEL_HUB í™•ì¸
        2-A. True â†’ Hugging Face Hubì—ì„œ ë‹¤ìš´ë¡œë“œ
        2-B. False â†’ ë¡œì»¬ íŒŒì¼ ì‚¬ìš©
        3. ëª¨ë¸ ë¡œë“œ
        """
        
        # ì¤‘ë³µ ë¡œë“œ ë°©ì§€
        if self.model is not None:
            logger.info("ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
        
        try:
            # Model Hub ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ê²½ë¡œ ê²°ì •
            if self.config.USE_MODEL_HUB:
                # === Model Hubì—ì„œ ë‹¤ìš´ë¡œë“œ ===
                logger.info(f"ğŸ“¥ Model Hubì—ì„œ ë‹¤ìš´ë¡œë“œ: {self.config.MODEL_HUB_REPO}")
                
                from huggingface_hub import hf_hub_download
                
                model_path = hf_hub_download(
                    repo_id=self.config.MODEL_HUB_REPO,
                    filename=self.config.MODEL_HUB_FILENAME,
                    cache_dir=self.config.MODEL_CACHE_DIR,
                    local_dir=self.config.MODEL_CACHE_DIR,
                    local_dir_use_symlinks=False  # ì‹¬ë³¼ë¦­ ë§í¬ ëŒ€ì‹  ì‹¤ì œ ë³µì‚¬
                )
                
                logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {model_path}")
                
            else:
                # === ë¡œì»¬ íŒŒì¼ ì‚¬ìš© ===
                model_path = self.config.GGUF_MODEL_PATH
                
                if not os.path.exists(model_path):
                    raise FileNotFoundError(
                        f"âŒ ë¡œì»¬ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}\n"
                        f"   USE_MODEL_HUB=trueë¡œ ì„¤ì •í•˜ê±°ë‚˜ ëª¨ë¸ íŒŒì¼ì„ ì¤€ë¹„í•˜ì„¸ìš”."
                    )
                
                logger.info(f"ğŸ“‚ ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©: {model_path}")
            
            # === ê³µí†µ: ëª¨ë¸ ë¡œë“œ ===
            logger.info(f"ğŸš€ GGUF ëª¨ë¸ ë¡œë“œ ì¤‘...")
            logger.info(f"   GPU ë ˆì´ì–´: {self.n_gpu_layers}")
            logger.info(f"   ì»¨í…ìŠ¤íŠ¸: {self.n_ctx}")
            
            self.model = Llama(
                model_path=model_path,
                n_gpu_layers=self.n_gpu_layers,
                n_ctx=self.n_ctx,
                n_threads=self.n_threads,
                verbose=False,
            )
            
            logger.info("âœ… GGUF ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            
        except FileNotFoundError as e:
            logger.error(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            raise
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def format_prompt(
        self,
        question: str,
        context: Optional[str] = None,
        system_prompt: Optional[str] = None
    ) -> str:
        """
        Llama-3 Chat í…œí”Œë¦¿ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            context: ì„ íƒì  ì»¨í…ìŠ¤íŠ¸ (RAG ê²€ìƒ‰ ê²°ê³¼)
            system_prompt: ì„ íƒì  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        
        Returns:
            í¬ë§·ëœ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
        """
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        if system_prompt is None:
            system_prompt = self.system_prompt
            logger.warning("âš ï¸ system_promptê°€ None! ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©")
        else:
            # ë™ì  í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 150ìë§Œ)
            logger.info(f"âœ… ë™ì  í”„ë¡¬í”„íŠ¸ ì ìš©:\n{system_prompt[:150]}...")  # â† ì¶”ê°€
            
        # ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€
        if context is not None:
            user_message = f"ì°¸ê³  ë¬¸ì„œ:\n{context}\n\nì§ˆë¬¸: {question}"
        else:
            user_message = question
        
        # Llama-3 Chat í…œí”Œë¦¿ ì ìš©
        formatted_prompt = (
            f"<|start_header_id|>system<|end_header_id|>\n\n"
            f"{system_prompt}<|eot_id|>"
            f"<|start_header_id|>user<|end_header_id|>\n\n"
            f"{user_message}<|eot_id|>"
            f"<|start_header_id|>assistant<|end_header_id|>\n\n"
        )
        
        return formatted_prompt
    
    def generate(
        self,
        prompt: str,
        max_new_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
    ) -> str:
        """
        í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥ë°›ì•„ ì‘ë‹µ ìƒì„±
        
        Args:
            prompt: í¬ë§·ëœ í”„ë¡¬í”„íŠ¸
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒì„± ë‹¤ì–‘ì„±
            top_p: Nucleus sampling
        
        Returns:
            ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸
        
        Raises:
            RuntimeError: ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°
        """
        # ëª¨ë¸ ë¡œë“œ í™•ì¸
        if self.model is None:
            raise RuntimeError(
                "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_model()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”."
            )
        
        # íŒŒë¼ë¯¸í„° ì„¤ì •
        if max_new_tokens is None:
            max_new_tokens = self.max_new_tokens
        if temperature is None:
            temperature = self.temperature
        if top_p is None:
            top_p = self.top_p
        
        try:
            logger.info(f"ğŸ”„ ìƒì„± ì‹œì‘ (max_tokens={max_new_tokens}, temp={temperature})")
            start_time = time.time()
            
            # ìƒì„±
            output = self.model(
                prompt,
                max_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                echo=False,  # í”„ë¡¬í”„íŠ¸ ë°˜ë³µ ì•ˆ í•¨
                stop=["<|eot_id|>", "<|end_of_text|>"],  # ì¢…ë£Œ í† í°
            )
            
            elapsed = time.time() - start_time
            logger.info(f"âœ… ìƒì„± ì™„ë£Œ: {elapsed:.2f}ì´ˆ")
            
            # ì‘ë‹µ ì¶”ì¶œ
            response = output['choices'][0]['text'].strip()
            
            logger.info(f"ğŸ“ ì‘ë‹µ ê¸¸ì´: {len(response)} ê¸€ì")
            return response
            
        except Exception as e:
            logger.error(f"âŒ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise RuntimeError(f"í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def chat(
        self,
        question: str,
        context: Optional[str] = None,
        system_prompt=None,
        **kwargs
    ) -> str:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„± (í†µí•© ë©”ì„œë“œ)
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            context: ì„ íƒì  ì»¨í…ìŠ¤íŠ¸ (RAG ê²°ê³¼)
            **kwargs: generate() íŒŒë¼ë¯¸í„°
        
        Returns:
            ìƒì„±ëœ ì‘ë‹µ
        """
        # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ… (system_prompt ì „ë‹¬)
        formatted_prompt = self.format_prompt(
            question=question,
            context=context,
            system_prompt=system_prompt  # â† ì¶”ê°€!
        )
        
        # ì‘ë‹µ ìƒì„±
        response = self.generate(formatted_prompt, **kwargs)

        return response
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        ëª¨ë¸ ì •ë³´ ë°˜í™˜
        
        Returns:
            ëª¨ë¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        info = {
            "model_path": self.model_path,
            "n_gpu_layers": self.n_gpu_layers,
            "n_ctx": self.n_ctx,
            "n_threads": self.n_threads,
            "is_loaded": self.model is not None,
            "max_new_tokens": self.max_new_tokens,
            "temperature": self.temperature,
            "top_p": self.top_p,
        }
        
        return info
    
    def __repr__(self):
        return f"GGUFGenerator(model={self.model_path}, loaded={self.model is not None})"


# ===== GGUF RAGPipeline: chatbot_app.py í˜¸í™˜ìš© =====

class GGUFRAGPipeline:
    """
    GGUF ëª¨ë¸ ê¸°ë°˜ RAG íŒŒì´í”„ë¼ì¸
    
    RAGPipeline(API ë²„ì „)ê³¼ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ì—¬
    chatbot_app.pyì™€ í˜¸í™˜ë©ë‹ˆë‹¤.
    """
    
    def __init__(self, config=None, model: str = None, top_k: int = None):
        """
        ì´ˆê¸°í™”
        
        Args:
            config: RAGConfig ê°ì²´
            model: ëª¨ë¸ ì´ë¦„ (ì‚¬ìš© ì•ˆ í•¨, í˜¸í™˜ì„±ìš©)
            top_k: ê¸°ë³¸ ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜
        """
        # Config import (ì§€ì—° importë¡œ ìˆœí™˜ ì°¸ì¡° ë°©ì§€)
        from src.utils.config import RAGConfig
        from src.retriever.retriever import RAGRetriever
        
        self.config = config or RAGConfig()
        self.top_k = top_k or self.config.DEFAULT_TOP_K
        
        # ê²€ìƒ‰ ì„¤ì •
        self.search_mode = self.config.DEFAULT_SEARCH_MODE
        self.alpha = self.config.DEFAULT_ALPHA
        
        # Retriever ì´ˆê¸°í™”
        logger.info("RAGRetriever ì´ˆê¸°í™” ì¤‘...")
        self.retriever = RAGRetriever(config=self.config)
        
        # GGUFGenerator ì´ˆê¸°í™”
        logger.info("GGUFGenerator ì´ˆê¸°í™” ì¤‘...")
        self.generator = GGUFGenerator(
            model_path=self.config.GGUF_MODEL_PATH,
            n_gpu_layers=self.config.GGUF_N_GPU_LAYERS,
            n_ctx=self.config.GGUF_N_CTX,
            n_threads=self.config.GGUF_N_THREADS,
            max_new_tokens=self.config.GGUF_MAX_NEW_TOKENS,
            temperature=self.config.GGUF_TEMPERATURE,
            top_p=self.config.GGUF_TOP_P,
            system_prompt=self.config.SYSTEM_PROMPT
        )
        
        # ëª¨ë¸ ë¡œë“œ (ì‹œê°„ ì†Œìš”)
        logger.info("GGUF ëª¨ë¸ ë¡œë“œ ì¤‘...")
        self.generator.load_model()
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬
        self.chat_history: List[Dict] = []
        
        # ë§ˆì§€ë§‰ ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ (sources ë°˜í™˜ìš©)
        self._last_retrieved_docs = []
        
        logger.info("âœ… GGUFRAGPipeline ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   - ê²€ìƒ‰ ëª¨ë“œ: {self.search_mode}")
        logger.info(f"   - ê¸°ë³¸ top_k: {self.top_k}")
    
    def _retrieve_and_format(self, query: str) -> str:
        """ê²€ìƒ‰ ìˆ˜í–‰ ë° ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…"""
        # ê²€ìƒ‰ ëª¨ë“œì— ë”°ë¼ ë¬¸ì„œ ê²€ìƒ‰
        if self.search_mode == "embedding":
            docs = self.retriever.search(query, top_k=self.top_k)
        elif self.search_mode == "embedding_rerank":
            docs = self.retriever.search_with_rerank(query, top_k=self.top_k)
        elif self.search_mode == "hybrid":
            docs = self.retriever.hybrid_search(
                query, top_k=self.top_k, alpha=self.alpha
            )
        elif self.search_mode == "hybrid_rerank":
            docs = self.retriever.hybrid_search_with_rerank(
                query, top_k=self.top_k, alpha=self.alpha
            )
        else:
            docs = self.retriever.search(query, top_k=self.top_k)
        
        # ë§ˆì§€ë§‰ ê²€ìƒ‰ ê²°ê³¼ ì €ì¥
        self._last_retrieved_docs = docs
        
        # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        return self._format_context(docs)
    
    def _format_context(self, retrieved_docs: list) -> str:
        """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if not retrieved_docs:
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        context_parts = []
        for i, doc in enumerate(retrieved_docs, 1):
            context_parts.append(f"[ë¬¸ì„œ {i}]\n{doc['content']}\n")
        
        return "\n".join(context_parts)
    
    def _format_sources(self, retrieved_docs: list) -> list:
        """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ sources í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        sources = []
        for doc in retrieved_docs:
            source_info = {
                'content': doc['content'],
                'metadata': doc['metadata'],
                'filename': doc.get('filename', 'N/A'),
                'organization': doc.get('organization', 'N/A')
            }
            
            # ê²€ìƒ‰ ëª¨ë“œì— ë”°ë¼ ì ìˆ˜ í•„ë“œê°€ ë‹¤ë¦„
            if 'rerank_score' in doc:
                source_info['score'] = doc['rerank_score']
                source_info['score_type'] = 'rerank'
            elif 'hybrid_score' in doc:
                source_info['score'] = doc['hybrid_score']
                source_info['score_type'] = 'hybrid'
            elif 'relevance_score' in doc:
                source_info['score'] = doc['relevance_score']
                source_info['score_type'] = 'embedding'
            else:
                source_info['score'] = 0
                source_info['score_type'] = 'unknown'
            
            sources.append(source_info)
        
        return sources
    
    def _estimate_usage(self, query: str, answer: str) -> dict:
        """í† í° ì‚¬ìš©ëŸ‰ ì¶”ì •"""
        # ê°„ë‹¨í•œ ë‹¨ì–´ ìˆ˜ ê¸°ë°˜ ì¶”ì •
        prompt_tokens = len(query.split()) * 2
        completion_tokens = len(answer.split()) * 2
        
        return {
            'total_tokens': prompt_tokens + completion_tokens,
            'prompt_tokens': prompt_tokens,
            'completion_tokens': completion_tokens
        }
    
    def generate_answer(
        self,
        query: str,
        top_k: int = None,
        search_mode: str = None,
        alpha: float = None
    ) -> dict:
        """
        ë‹µë³€ ìƒì„± (chatbot_app.py í˜¸í™˜ ë©”ì¸ ë©”ì„œë“œ)
        
        Args:
            query: ì§ˆë¬¸
            top_k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            search_mode: ê²€ìƒ‰ ëª¨ë“œ
            alpha: ì„ë² ë”© ê°€ì¤‘ì¹˜
        
        Returns:
            dict: answer, sources, search_mode, usage, elapsed_time, used_retrieval
        """
        try:
            start_time = time.time()
            
            # íŒŒë¼ë¯¸í„° ì„¤ì • (ê²€ìƒ‰ ì „ì— ë¨¼ì € ì„¤ì •)
            if top_k is not None:
                self.top_k = top_k
            if search_mode is not None:
                self.search_mode = search_mode
            if alpha is not None:
                self.alpha = alpha

            # ===== Routerë¡œ ê²€ìƒ‰ ì—¬ë¶€ ê²°ì • =====
            router = QueryRouter()
            classification = router.classify(query)
            query_type = classification['type']  # 'greeting'/'thanks'/'document'/'out_of_scope'
            
            logger.info(f"ğŸ“ ë¶„ë¥˜: {query_type} "
                f"(ì‹ ë¢°ë„: {classification['confidence']:.2f})")
            
            # 2. íƒ€ì…ë³„ ì²˜ë¦¬
            if query_type in ['greeting', 'thanks', 'out_of_scope']:
                # ê²€ìƒ‰ ìŠ¤í‚µ
                context = None
                used_retrieval = False
                self._last_retrieved_docs = []
                
                # ë™ì  í”„ë¡¬í”„íŠ¸ ì„ íƒ
                system_prompt = PromptManager.get_prompt(query_type)
                logger.info(f"â­ï¸ RAG ìŠ¤í‚µ: {query_type}")
            
            elif query_type == 'document':
                # RAG ìˆ˜í–‰
                context = self._retrieve_and_format(query)
                used_retrieval = True
                
                # ë™ì  í”„ë¡¬í”„íŠ¸ (context í¬í•¨)
                system_prompt = PromptManager.get_prompt('document')
                logger.info(f"ğŸ” RAG ìˆ˜í–‰: {len(self._last_retrieved_docs)}ê°œ ë¬¸ì„œ")
            
            # 3. ë‹µë³€ ìƒì„± (system_prompt ì „ë‹¬)
            answer = self.generator.chat(
                question=query,
                context=context,
                system_prompt=system_prompt  # â† ì¶”ê°€!
            )
            
            elapsed_time = time.time() - start_time
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            self.chat_history.append({"role": "user", "content": query})
            self.chat_history.append({"role": "assistant", "content": answer})
            
            # ê²°ê³¼ ë°˜í™˜ (RAGPipelineê³¼ ë™ì¼ í˜•ì‹)
            return {
                'answer': answer,
                'sources': self._format_sources(self._last_retrieved_docs),
                'used_retrieval': used_retrieval,
                'query_type': query_type,  # â† ì¶”ê°€!
                'search_mode': self.search_mode if used_retrieval else 'direct',
                'routing_info': classification,
                'elapsed_time': elapsed_time,
                'usage': self._estimate_usage(query, answer)
            }
        
        except Exception as e:
            logger.error(f"âŒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}") from e
    
    def chat(self, query: str) -> str:
        """ê°„ë‹¨í•œ ëŒ€í™” ì¸í„°í˜ì´ìŠ¤"""
        result = self.generate_answer(query)
        return result['answer']
    
    def clear_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.chat_history = []
        logger.info("ğŸ—‘ï¸ ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def get_history(self) -> List[Dict]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        return self.chat_history.copy()
    
    def set_search_config(
        self,
        search_mode: str = None,
        top_k: int = None,
        alpha: float = None
    ):
        """ê²€ìƒ‰ ì„¤ì • ë³€ê²½"""
        if search_mode is not None:
            self.search_mode = search_mode
        if top_k is not None:
            self.top_k = top_k
        if alpha is not None:
            self.alpha = alpha
        
        logger.info(
            f"ğŸ”§ ê²€ìƒ‰ ì„¤ì • ë³€ê²½: mode={self.search_mode}, "
            f"top_k={self.top_k}, alpha={self.alpha}"
        )


# í…ŒìŠ¤íŠ¸ìš©
if __name__ == "__main__":
    from src.utils.config import RAGConfig
    
    config = RAGConfig()
    
    # GGUFRAGPipeline ì´ˆê¸°í™”
    pipeline = GGUFRAGPipeline(config=config)
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_questions = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "ë³¸ ì‚¬ì—…ì˜ ì˜ˆì‚° ë²”ìœ„ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "ê³ ë§ˆì›Œìš”!"
    ]
    
    for question in test_questions:
        print("\n" + "="*50)
        print("í…ŒìŠ¤íŠ¸ ì§ˆë¬¸:", question)
        print("="*50)
        
        result = pipeline.generate_answer(question)
        
        print(f"\në¼ìš°íŒ…: {result['routing_info']['route']}")
        print(f"ê²€ìƒ‰ ì‚¬ìš©: {result['used_retrieval']}")
        print("\nì‘ë‹µ:")
        print(result['answer'])
        print(f"\nì†Œìš” ì‹œê°„: {result['elapsed_time']:.2f}ì´ˆ")
        print(f"ì°¸ê³  ë¬¸ì„œ: {len(result['sources'])}ê°œ")
        print("="*50)